# 1
## 1
$|\Phi(x)|=|\theta^{(0)}|=l$

$\theta^{(0)}=(0..0)$

$\theta^{(j)}=\sum^m_{i=1}\alpha_i^{(j)}\Phi(x_i), \alpha_i^{(j)}=(\alpha_0^{(j)}, \alpha_m^{(j)})$

## 2

$h_{\theta^{(i)}}(x)=g((\sum^m_{i=1}\alpha_i\Phi(x_i))^T\Phi(x))=$
$g((\sum^m_{i=1}\alpha_i\Phi(x_i),\Phi(x)))=$
$g(\sum^m_{i=1}(\alpha_i\Phi(x_i),\Phi(x)))=$
$g(\sum^m_{i=1}\alpha_i(\Phi(x_i),\Phi(x)))=$
$g(\sum^m_{i=1}\alpha_iK(x_i,x))$

## 3

$a=sign(\sum^m_{j=1}\alpha_iK(x_j, x_{i+1}));$

__if__ $a \ne y_{i+1}${

$\alpha_{i+1} = \alpha_{i+1} + s*(y_{i+1}-a)$ # s - learning rate

}

# 2

perceptron_dot_output.png - линейное

perceptron_rbf_output.png - гауссово

# 3

Линейное ядро показывает плохую точность т.к данные невозможно разбить на нужные классы с помощью прямой. 
